{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pA6CmJ4fW-X"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/nvidia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKk7_4JpfW-Z"
      },
      "source": [
        "# Using Nvidia Embeddings and Models\n",
        "\n",
        "This notebook demonstrates how to set up a simple RAG pipeline using [NVIDIA AI Foundation Models](https://catalog.ngc.nvidia.com). At the end of this notebook, you will have a functioning Question/Answer pipeline that can answer questions using your supplied documents, powered by Astra DB, LangChain, and NVIDIA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e5bE-wefW-Z"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "You will need a vector-enabled Astra database and an NVIDIA NGC Account.\n",
        "\n",
        "* Create an [Astra vector database](https://docs.datastax.com/en/astra-serverless/docs/getting-started/create-db-choices.html).\n",
        "* Within your database, create an [Astra DB Access Token](https://docs.datastax.com/en/astra-serverless/docs/manage/org/manage-tokens.html) with Database Administrator permissions.\n",
        "* Get your Astra DB Endpoint:\n",
        "  * `https://<ASTRA_DB_ID>-<ASTRA_DB_REGION>.apps.astra.datastax.com`\n",
        "* Create an [NVIDIA NGC Account](https://catalog.ngc.nvidia.com/)\n",
        "  * Once signed in, navigate to `Catalog > AI Foundation Models > (Model)`\n",
        "  * In the model page, select the `API` tab, then `Generate Key`\n",
        "\n",
        "See the [Prerequisites](https://docs.datastax.com/en/ragstack/docs/prerequisites.html) page for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STMENH52fW-a"
      },
      "source": [
        "## Setup\n",
        "`ragstack-ai` includes all the packages you need to build a RAG pipeline.\n",
        "\n",
        "`langchain-nvidia-ai-endpoints` includes the NVIDIA models.\n",
        "\n",
        "`datasets` is used to import a sample dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbmake": {
          "post_cell_execute": [
            "from conftest import before_notebook",
            "before_notebook()"
          ]
        },
        "id": "jXGso-cqfW-a"
      },
      "outputs": [],
      "source": [
        "! pip install -qU ragstack-ai langchain-nvidia-ai-endpoints datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "nbmake": {
          "post_cell_execute": [
            "import string\n",
            "import random\n",
            "collection = ''.join(random.choice(string.ascii_lowercase) for _ in range(8))\n"
          ]
        },
        "tags": [
          "skip-execution"
        ],
        "id": "UBYOqKojfW-b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Enter your settings for Astra DB and NVidia:\n",
        "os.environ[\"ASTRA_DB_API_ENDPOINT\"] = input(\"Enter your Astra DB Endpoint: \")\n",
        "os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"] = getpass(\"Enter your Astra DB Token: \")\n",
        "os.environ[\"NVIDIA_API_KEY\"] = getpass(\"Enter your NVidia API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "bRycDrjzfW-b"
      },
      "outputs": [],
      "source": [
        "# Collections are where documents are stored. ex: test\n",
        "collection = \"nvidia_test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txoj9qAifW-c"
      },
      "source": [
        "## Create RAG Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWngszc9fW-c"
      },
      "source": [
        "### Embedding Model and Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms4VxQw4fW-c"
      },
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "\n",
        "embedding = NVIDIAEmbeddings(model=\"nvolveqa_40k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "03udCKc2fW-c",
        "outputId": "4e91405e-5cf2-4a53-aef0-d6a1ed4b8297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Astra vector store configured\n"
          ]
        }
      ],
      "source": [
        "from langchain_astradb import AstraDBVectorStore\n",
        "import os\n",
        "\n",
        "vstore = AstraDBVectorStore(\n",
        "    collection_name=collection,\n",
        "    embedding=embedding,\n",
        "    token=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\"),\n",
        "    api_endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\"),\n",
        ")\n",
        "print(\"Astra vector store configured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejRhdI9wfW-d",
        "outputId": "71e275ca-90ef-4515-aec4-037e08da4181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An example entry:\n",
            "{'author': 'aristotle', 'quote': 'Love well, be loved and do something of value.', 'tags': 'love;ethics'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load a sample dataset\n",
        "philo_dataset = load_dataset(\"datastax/philosopher-quotes\")[\"train\"]\n",
        "print(\"An example entry:\")\n",
        "print(philo_dataset[16])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5Wgmq5QfW-d"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "# Constructs a set of documents from your data. Documents can be used as inputs to your vector store.\n",
        "docs = []\n",
        "for entry in philo_dataset:\n",
        "    metadata = {\"author\": entry[\"author\"]}\n",
        "    if entry[\"tags\"]:\n",
        "        # Add metadata tags to the metadata dictionary\n",
        "        for tag in entry[\"tags\"].split(\";\"):\n",
        "            metadata[tag] = \"y\"\n",
        "    # Create a LangChain document with the quote and metadata tags\n",
        "    doc = Document(page_content=entry[\"quote\"], metadata=metadata)\n",
        "    docs.append(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbmake": {
          "post_cell_execute": [
            "assert len(inserted_ids) > 0"
          ]
        },
        "id": "60pMnAS9fW-d",
        "outputId": "674e1832-e897-41b9-d019-d1d17c791383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Inserted 450 documents.\n"
          ]
        }
      ],
      "source": [
        "# Create embeddings by inserting your documents into the vector store.\n",
        "inserted_ids = vstore.add_documents(docs)\n",
        "print(f\"\\nInserted {len(inserted_ids)} documents.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpXWtFnCfW-d"
      },
      "outputs": [],
      "source": [
        "# Checks your collection to verify the documents are embedded.\n",
        "print(vstore.astra_db.collection(collection).find())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3yNHbpUfW-e"
      },
      "source": [
        "### QA Retrieval\n",
        "\n",
        "Retrieve context from your vector database, and pass it to the NVIDIA model with a prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zvvxs7rhfW-e",
        "outputId": "b683a2d9-7f75-46de-bf50-a2d751111050"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In the given context, philosophers are most concerned with the subject of philosophy itself. Aristotle discusses philosophy as starting with wonder and also mentions that it can make people sick. Hegel, on the other hand, discusses the relationship between knowledge and education. Therefore, it can be inferred that philosophers are concerned with understanding the world, knowledge, and its implications on individuals and society.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "retriever = vstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "Answer the question based only on the supplied context. If you don't know the answer, say you don't know the answer.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Your answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "model = ChatNVIDIA(model=\"mixtral_8x7b\")\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke(\"In the given context, what subject are philosophers most concerned with?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiVtIR1UfW-e"
      },
      "outputs": [],
      "source": [
        "# Add your questions here!\n",
        "# chain.invoke(\"<your question>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErpakOKefW-e"
      },
      "source": [
        "## Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbmake": {
          "post_cell_execute": [
            "# Deletes collection for test suite to allow each test to run with a fresh collection",
            "vstore.delete_collection()"
          ]
        },
        "id": "o3kLO3ASfW-e"
      },
      "outputs": [],
      "source": [
        "# WARNING: This will delete the collection and all documents in the collection\n",
        "# vstore.delete_collection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2rV_IZCfW-e"
      },
      "source": [
        "You now have a functional RAG pipeline powered by NVIDIA! NVIDIA offers many different model types suited for different problems. Check out the [catalog](https://catalog.ngc.nvidia.com) for more."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}